
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Validator High-Availability &#8212; Validator Operations Guide  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="HSM for Signing" href="hsm.html" />
    <link rel="prev" title="Tendermint P2P Layer" href="peers.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="validator-high-availability">
<h1>Validator High-Availability<a class="headerlink" href="#validator-high-availability" title="Permalink to this headline">¶</a></h1>
<p>Tendermint is a globally consistent, byzantine-fault tolerant consensus-based replicated
state machine - the gold standard in distributed systems.</p>
<p>A single validator, however, is not highly available. It doesn’t need to be - the network
tolerates single validator failures just fine.</p>
<p>However, as the validator operator, things look different and your tolerance for single validator
failures is pretty low. Sooner or later, every validator operator is confronted with the
question of making their validator fault-tolerant. Unfortunately, there’s no established
procedure for this and each team is forced to build their own high availability solution.</p>
<p>This article portrays some possible HA topologies and their pros and cons.</p>
<p>Running a Tendermint validator is - like any distributed system - a delicate balance between
ensuring availability and consistency/correctness (availability as in “up and signing blocks”,
and consistency as in “no byzantine behavior”).</p>
<p>As is well known, the <a class="reference external" href="https://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a> states that you
can only pick two of partition tolerance, consistency and availability. Since network partitions
are inevitable in any distributed system, you can’t pick CA, and the only choices are AP and CP.</p>
<p>With Cosmos validators, the penalty for inconsistency (double signing!) is much harsher than
unavailability (you even get a few minutes to fix it before you are slashed). This means that
the only reasonable choice is a CP system - if there’s even the slightest possibility that more
than one node is active at the same time, it needs to sacrifice availability (i.e. stop signing).</p>
<div class="section" id="single-node-validator">
<h2>Single Node Validator<a class="headerlink" href="#single-node-validator" title="Permalink to this headline">¶</a></h2>
<p>Since inconsistency is so dangerous, running a single-node validator is a very reasonable choice.
Modern datacenter hardware, in a modern datacenter, with redundant networking, designed by an
experienced systems architect, has a very low failure rate sufficient for many low-stakes validators.</p>
<p>As any sysadmin will tell you, badly designed high availability setups tend to fail at a much
higher frequency than reliable single-node ones.</p>
<p>By never running two nodes at the same time, the risk of double signing is very low - one would
have to trick your replacement validator into signing an earlier block while it recovers or
exploit a vulnerability in Tendermint.</p>
<p>However, if the hardware or datacenter does fail in a catastropic way, you will be down for an
extended period of time until you have got replacement hardware in place and re-synced. While
acceptable for low-stakes validators, most commercial validator operations won’t be able to accept
this risk. Even redundant power and networking setups have a non-zero chance of failure, in fact,
most systems aren’t fully isolated and failures often tend to be correlated.</p>
<p>(True story: your router has redundant PSUs? Oops, both fans just simultaneously ingested a
packaging foil that someone left in the rack).</p>
</div>
<div class="section" id="active-standby-validator">
<h2>Active-Standby Validator<a class="headerlink" href="#active-standby-validator" title="Permalink to this headline">¶</a></h2>
<p>The next obvious step following a single-node validator is an active-standby setup with
manually promoted slaves. If the active node dies, a sysadmin gets paged, ensures the active node
is actually dead, and then manually promotes the standby node. You would be surprised how many
large SaaS businesses rely on a single beefy MySQL or Postgres server with manual failover!</p>
<p>Both nodes would be identically configured, with the same validator key, mnode key and moniker.</p>
<p>This requires an on-call rotation with very low response times, which is expensive - you’re
basically paying someone to sit at home all week, ready to react within minutes (most companies
which do this do a follow-the-sun rotation with offices on three continents). It’s also susceptible
to human error in determining node states.</p>
<p>Fortunately, active-standby setups are very common and there’s a lot of tooling for automated
failover. The state-of-the art solution is <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/high_availability_add-on_overview/ch-introduction-haao">Pacemaker/Corosync</a>, superseding <a class="reference external" href="https://web.archive.org/web/20180829165659/http://www.linux-ha.org/wiki/Main_Page">heartbeat</a>. Both
were initially designed for VM clusters, which have very similar requirements to validators: no
VM may ever run twice on the cluster, since it would result in immediate data corruption with both
running from the same storage.</p>
<p>While Pacemaker/Corosync supports running with only two nodes, this is essentially a CA setup and
<em>will not</em> survive a network partition. This is common (and reasonable) for router/firewall
clusters, with devices close to each other and redundant network interfaces and cabling, but we
do not recommend a two-node cluster for validators due to the obvious risks e.g. of reaching
a <em>split-brain</em> scenario where both nodes go active at the same time.</p>
<p>Best practice for Pacemaker is a three node deployment - one active node, one standby node and
a third quorum-only/witness node. Pacemaker will only <em>enable a resource</em> (i.e. start the validator
service) if it can establish a quorum, and will <em>self-fence</em> (i.e. kill the validator service;
the act of reliably excluding a broken node from the cluster is called <em>fencing</em>) if it loses quorum.</p>
<p>However, even with a quorum, self-fencing can fail. There are many edge cases around the interaction
between pacemaker and the resources it protects. For instance, if pacemaker crashes,
the node might be considered dead, but the validator service is still running, or your system’s
service manager reporting the validator service as down when it’s actually still running.</p>
<p>To prevent these cases, a so-called <em>fencing device</em> like a mechanical relay or out-of-band management
interface (IPMI, iLO, …) is necessary which interrupts power for the other node to ensure that
it’s really <em>really</em> down (also called STONITH - “shoot the other node in the head”).</p>
<p>A Pacemaker/Corosync is a good option for validator high availability, but involves risks and drawbacks:</p>
<ul class="simple">
<li>Pacemaker/Corosync is complicated to operate and has many edge cases. We recommend reading
the Red Hat guide linked above which details common Pacemaker setups, and hiring
someone who has experience with Pacemaker in critical production environments.</li>
<li>Fencing is failure-prone - it’s actually <em>really</em> hard to ensure a node is down and won’t
ever switch back on, either by resetting spontaneously or by operator mistake, and the fencing
mechanisms needs to be tested extensively to reduce chance of failure.</li>
<li>Fencing and failover based on network load balancers or IP failover are unlikely to work as
expected due to stateful connections and the P2P nature of the Tendermint network.
You need to ensure that the actual processes (and ideally the physical hosts) are stopped.</li>
<li>You need to synchronize block height/round/step of the last signature to your standby node or risk
double signing due to a race condition during failover or someone tricking your node. Do not use
shared or replicated storage - it’s hard to reason about and introduces an additional point of failure
(see our response in <a class="reference external" href="https://forum.cosmos.network/t/backing-up-validator-server-physical-data-center/751/2?u=certus_zl">this forum thread</a>).</li>
<li>Pacemaker/Corosync aren’t designed to work across high latency networks, so this setup won’t scale
beyond a single data center or even metro network (the corosync protocol expects latencies &lt;2ms).</li>
</ul>
</div>
<div class="section" id="active-active-validator">
<h2>Active-Active Validator<a class="headerlink" href="#active-active-validator" title="Permalink to this headline">¶</a></h2>
<p>Certus One has settled for an active-active validator setup. We run multiple validators, with the
same key, <em>at the same time</em>, in multiple geographically distributed data centers.
We built our own Raft-based consensus layer on top of the Tendermint core by implementing our own
PrivValidator wrapper, which forces all signatures through a full Raft consensus.</p>
<p>While we’ve seen some proposals <a class="footnote-reference" href="#ha1" id="id1">[1]</a> <a class="footnote-reference" href="#ha2" id="id2">[2]</a> to use the Tendermint protocol itself to provide
consensus, we decided it would introduce unnecessary complexity where a (comparably) simple protocol
like Raft would suffice - we fortunately don’t need byzantine fault tolerance here.</p>
<p>This removes the brittleness of active-passive setups and the synchrony requirements of the corosync
protocol. All of our validators are synchronized with the network at the current block height, ready
to sign, while reliably preventing double signing through the Raft log.</p>
<p>This provides guaranteed consistency as well as very high availability (the CAP theorem still
wins, though - there’s a small window of time where a node can crash just before submitting a
signature to the network, where we cannot reliably retry the operation since we can’t know for
sure whether it succeeded; this is deliberate and cannot be fixed without risking consistency).</p>
<p>While our active-active technology - called JANUS - currently isn’t an open source project, we
open-sourced all of its dependencies and the testing framework <a class="footnote-reference" href="#testing" id="id3">[3]</a> we use. We’re closely
following upstream discussions and may decide to open source JANUS later.</p>
<p>We believe that active-active validator setups are the best way going forward,
and look forward to contribute to the community discussions regarding active-active setups.</p>
<table class="docutils footnote" frame="void" id="ha1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[1]</a></td><td><a class="reference external" href="https://github.com/tendermint/tendermint/issues/1758">https://github.com/tendermint/tendermint/issues/1758</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="ha2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[2]</a></td><td><a class="reference external" href="https://github.com/tendermint/kms/issues/29">https://github.com/tendermint/kms/issues/29</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="testing" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[3]</a></td><td><a class="reference internal" href="testing.html"><span class="doc">Testing your tooling</span></a></td></tr>
</tbody>
</table>
<div class="section" id="network-topology">
<h3>Network topology<a class="headerlink" href="#network-topology" title="Permalink to this headline">¶</a></h3>
<p>Raft is usually deployed within a single data center, however, the protocol does not <em>require</em> low
latencies and works just fine with higher latencies (at the expense of elections and consensus
read/writes taking a multiple of the worst latency in the cluster), assuming proper tuning and
timeouts. The acceptable latency depends on the block times in the Tendermint network. We’re
running all nodes within central Europe with no node being farther away than 50ms to ensure that
a consensus read completes within &lt;1s.</p>
<p>We run at most one validator per data center, with no more than <em>n</em> validators per autonomous system,
where <em>n</em> is the number of nodes that the cluster can lose without losing consensus. All nodes are
BGP multi-homed with multiple transit providers.</p>
<p>We mapped out all routes between the data centers and ensured - through either private peering
agreements or BGP traffic engineering - that even failure of critical transit networks or internet
exchanges will not result in loss of consensus.</p>
<p>This allows us to survive the failure of multiple data centers, whole autonomous systems, as well
as internet exchanges while at most losing one block.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/brand.png" alt="Logo"/>
            </a></p><h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring, Alerting and Instrumentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="peers.html">Tendermint P2P Layer</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Validator High-Availability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#single-node-validator">Single Node Validator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#active-standby-validator">Active-Standby Validator</a></li>
<li class="toctree-l2"><a class="reference internal" href="#active-active-validator">Active-Active Validator</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hsm.html">HSM for Signing</a></li>
<li class="toctree-l1"><a class="reference internal" href="key_management.html">Key Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Testing your tooling</a></li>
<li class="toctree-l1"><a class="reference internal" href="building.html">Building your tools and Cosmos</a></li>
<li class="toctree-l1"><a class="reference internal" href="business_continuity.html">Business Continuity</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="peers.html" title="previous chapter">Tendermint P2P Layer</a></li>
      <li>Next: <a href="hsm.html" title="next chapter">HSM for Signing</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, Certus One GmbH.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.5</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="_sources/validator_ha.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>